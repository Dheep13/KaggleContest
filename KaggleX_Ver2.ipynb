{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPzHatI6zGNXYSq1dITk5hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"7new4wl_vmpH","executionInfo":{"status":"ok","timestamp":1717218570545,"user_tz":-330,"elapsed":11489,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"7c414e93-9db7-4b54-91d8-b4e24d667b38"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2637d59a-05f2-487d-a535-617cf75e4e31\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2637d59a-05f2-487d-a535-617cf75e4e31\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"deepanshanmugam\",\"key\":\"b6ba4dd7b52d960f39aff9202855df43\"}'}"]},"metadata":{},"execution_count":2}],"source":["from google.colab import files\n","\n","# This will prompt you to select the kaggle.json file\n","files.upload()\n"]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n"],"metadata":{"id":"1K93XjvTwEpP","executionInfo":{"status":"ok","timestamp":1717218570545,"user_tz":-330,"elapsed":4,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# !pip install kaggle\n","!kaggle competitions download -c kagglex-cohort4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cbjT1IxwI1z","executionInfo":{"status":"ok","timestamp":1717218572743,"user_tz":-330,"elapsed":2200,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"21dd9fd1-fa4e-475a-c030-210b689372d8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading kagglex-cohort4.zip to /content\n"," 92% 2.00M/2.17M [00:00<00:00, 2.95MB/s]\n","100% 2.17M/2.17M [00:00<00:00, 2.75MB/s]\n"]}]},{"cell_type":"code","source":["!unzip -q kagglex-cohort4.zip\n"],"metadata":{"id":"w86ccyl0wL5H","executionInfo":{"status":"ok","timestamp":1717218572743,"user_tz":-330,"elapsed":7,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# View the files in the dataset directory\n","import os\n","\n","dataset_dir = '/content'\n","for dirname, _, filenames in os.walk(dataset_dir):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QupkKRNqwPuq","executionInfo":{"status":"ok","timestamp":1717218572743,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"12431254-bfb7-42dd-a82c-d0777e8d7537"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/train.csv\n","/content/kagglex-cohort4.zip\n","/content/test.csv\n","/content/kaggle.json\n","/content/sample_submission.csv\n","/content/.config/gce\n","/content/.config/config_sentinel\n","/content/.config/default_configs.db\n","/content/.config/.last_update_check.json\n","/content/.config/.last_opt_in_prompt.yaml\n","/content/.config/active_config\n","/content/.config/.last_survey_prompt.yaml\n","/content/.config/logs/2024.05.30/13.23.26.998698.log\n","/content/.config/logs/2024.05.30/13.36.15.385607.log\n","/content/.config/logs/2024.05.30/13.30.33.059431.log\n","/content/.config/logs/2024.05.30/13.30.21.274210.log\n","/content/.config/logs/2024.05.30/13.36.16.059291.log\n","/content/.config/logs/2024.05.30/13.36.03.155708.log\n","/content/.config/configurations/config_default\n","/content/sample_data/anscombe.json\n","/content/sample_data/README.md\n","/content/sample_data/california_housing_test.csv\n","/content/sample_data/california_housing_train.csv\n","/content/sample_data/mnist_test.csv\n","/content/sample_data/mnist_train_small.csv\n"]}]},{"cell_type":"code","source":["\n","###Version 1 - scaling and preprocessing done for price and id\n","# import xgboost as xgb\n","# from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n","# from sklearn.metrics import mean_squared_error\n","# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","# import pandas as pd\n","# import numpy as np\n","\n","# # Load datasets\n","# train_df = pd.read_csv('/content/train.csv')\n","# test_df = pd.read_csv('/content/test.csv')\n","\n","# # Feature Engineering\n","# # Example of creating new features\n","# train_df['age'] = 2024 - train_df['model_year']  # Assuming the year column exists and 2024 is the current year\n","# test_df['age'] = 2024 - test_df['model_year']\n","\n","# # Handle missing values for numerical columns\n","# train_df.fillna(train_df.select_dtypes(include=['number']).mean(), inplace=True)\n","# test_df.fillna(test_df.select_dtypes(include=['number']).mean(), inplace=True)\n","\n","# # Ensure 'age' column is included in numerical columns\n","# numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# # Remove redundant or unimportant features\n","# # Drop columns with low variance (only on numerical columns)\n","# low_variance_cols = train_df[numerical_cols].var()[train_df[numerical_cols].var() < 0.1].index.tolist()\n","# train_df.drop(columns=low_variance_cols, inplace=True)\n","# test_df.drop(columns=low_variance_cols, inplace=True)\n","\n","# # Update numerical columns after dropping low variance columns\n","# numerical_cols = [col for col in numerical_cols if col not in low_variance_cols]\n","\n","# # Drop highly correlated features (only on numerical columns)\n","# correlation_matrix = train_df[numerical_cols].corr().abs()\n","# upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n","# high_correlation_cols = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n","# train_df.drop(columns=high_correlation_cols, inplace=True)\n","# test_df.drop(columns=high_correlation_cols, inplace=True)\n","\n","# # Update numerical columns after dropping highly correlated columns\n","# numerical_cols = [col for col in numerical_cols if col not in high_correlation_cols]\n","\n","# # Drop the 'clean_title' column\n","# train_df.drop(columns=['clean_title'], inplace=True)\n","# test_df.drop(columns=['clean_title'], inplace=True)\n","\n","# # Transform existing features\n","# # Log transform skewed features (only on numerical columns, excluding 'price')\n","# skewed_cols = train_df[numerical_cols].skew().abs() > 0.75\n","# skewed_features = skewed_cols[skewed_cols].index.tolist()\n","# if 'price' in skewed_features:\n","#     skewed_features.remove('price')  # Ensure 'price' is not included\n","# train_df[skewed_features] = np.log1p(train_df[skewed_features])\n","# test_df[skewed_features] = np.log1p(test_df[skewed_features])\n","\n","# # Identify categorical columns\n","# categorical_cols = train_df.select_dtypes(include=['object']).columns\n","\n","# # Apply OneHotEncoder to categorical columns\n","# ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","# train_encoded = ohe.fit_transform(train_df[categorical_cols])\n","# test_encoded = ohe.transform(test_df[categorical_cols])\n","\n","# # Convert encoded data to DataFrames\n","# train_encoded_df = pd.DataFrame(train_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","# test_encoded_df = pd.DataFrame(test_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","\n","# # Concatenate encoded columns back to the original DataFrames\n","# train_df = pd.concat([train_df.drop(columns=categorical_cols), train_encoded_df], axis=1)\n","# test_df = pd.concat([test_df.drop(columns=categorical_cols), test_encoded_df], axis=1)\n","\n","# # Update numerical columns after adding encoded columns\n","# numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# # Ensure 'price' is not included in numerical columns for scaling\n","# if 'price' in numerical_cols:\n","#     numerical_cols.remove('price')\n","\n","# # Apply log transformation to the target variable\n","# train_df['price'] = np.log1p(train_df['price'])\n","\n","# upper_bound = 95000  # Calculated upper bound\n","# train_df['price'] = train_df['price'].clip(upper=upper_bound)\n","\n","\n","# # Feature Scaling\n","# scaler = StandardScaler()\n","# train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n","# test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n","\n","# # Split the training data into features and target variable\n","# X = train_df.drop(columns=['price'])\n","# y = train_df['price']\n","\n","# # Split data into training and validation sets\n","# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# # Use a smaller subset of the data for initial hyperparameter tuning\n","# X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n","\n","# # Hyperparameter Tuning with RandomizedSearchCV\n","# param_dist = {\n","#     'n_estimators': [100, 200, 300],\n","#     'max_depth': [3, 6, 9],\n","#     'learning_rate': [0.01, 0.1, 0.3],\n","# }\n","\n","# model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', device='cuda')\n","\n","# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='neg_mean_squared_error', cv=3, n_iter=10, verbose=1, n_jobs=-1, random_state=42)\n","# random_search.fit(X_train_small, y_train_small)\n","\n","# best_params = random_search.best_params_\n","# print(\"Best parameters:\", best_params)\n","\n","# # Train model with best parameters on full dataset\n","# model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', tree_method='hist', device='cuda')\n","# model.fit(X_train, y_train)\n","\n","# # Evaluate the model\n","# y_pred = model.predict(X_val)\n","# rmse = mean_squared_error(y_val, y_pred, squared=False)  # squared=False to get RMSE directly\n","# print(f'Validation RMSE: {rmse}')\n","\n","# # Cross-validation\n","# cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","# cv_rmse = np.sqrt(-cv_scores)\n","# print(f'Cross-validated RMSE: {cv_rmse.mean()}')\n","\n"],"metadata":{"id":"E_FBuZo-vqV9","executionInfo":{"status":"ok","timestamp":1717218572743,"user_tz":-330,"elapsed":3,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","import pandas as pd\n","import numpy as np\n","\n","# Load datasets\n","train_df = pd.read_csv('/content/train.csv')\n","test_df = pd.read_csv('/content/test.csv')\n","\n","# Store the 'id' column from test data\n","test_ids = test_df['id']\n","\n","# Feature Engineering\n","# Example of creating new features\n","train_df['age'] = 2024 - train_df['model_year']  # Assuming the year column exists and 2024 is the current year\n","test_df['age'] = 2024 - test_df['model_year']\n","\n","# Handle missing values for numerical columns\n","train_df.fillna(train_df.select_dtypes(include=['number']).mean(), inplace=True)\n","test_df.fillna(test_df.select_dtypes(include=['number']).mean(), inplace=True)\n","\n","# Ensure 'age' column is included in numerical columns\n","numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# Remove redundant or unimportant features\n","# Drop columns with low variance (only on numerical columns)\n","low_variance_cols = train_df[numerical_cols].var()[train_df[numerical_cols].var() < 0.1].index.tolist()\n","train_df.drop(columns=low_variance_cols, inplace=True)\n","test_df.drop(columns=low_variance_cols, inplace=True)\n","\n","# Update numerical columns after dropping low variance columns\n","numerical_cols = [col for col in numerical_cols if col not in low_variance_cols]\n","\n","# Drop highly correlated features (only on numerical columns)\n","correlation_matrix = train_df[numerical_cols].corr().abs()\n","upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n","high_correlation_cols = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n","train_df.drop(columns=high_correlation_cols, inplace=True)\n","test_df.drop(columns=high_correlation_cols, inplace=True)\n","\n","# Update numerical columns after dropping highly correlated columns\n","numerical_cols = [col for col in numerical_cols if col not in high_correlation_cols]\n","\n","# Drop the 'clean_title' column\n","train_df.drop(columns=['clean_title'], inplace=True)\n","test_df.drop(columns=['clean_title'], inplace=True)\n","\n","# Transform existing features\n","# Log transform skewed features (only on numerical columns, excluding 'price')\n","skewed_cols = train_df[numerical_cols].skew().abs() > 0.75\n","skewed_features = skewed_cols[skewed_cols].index.tolist()\n","if 'price' in skewed_features:\n","    skewed_features.remove('price')  # Ensure 'price' is not included\n","train_df[skewed_features] = np.log1p(train_df[skewed_features])\n","test_df[skewed_features] = np.log1p(test_df[skewed_features])\n","\n","# Identify categorical columns\n","categorical_cols = train_df.select_dtypes(include=['object']).columns\n","\n","# Apply OneHotEncoder to categorical columns\n","ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","train_encoded = ohe.fit_transform(train_df[categorical_cols])\n","test_encoded = ohe.transform(test_df[categorical_cols])\n","\n","# Convert encoded data to DataFrames\n","train_encoded_df = pd.DataFrame(train_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","test_encoded_df = pd.DataFrame(test_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","\n","# Concatenate encoded columns back to the original DataFrames\n","train_df = pd.concat([train_df.drop(columns=categorical_cols), train_encoded_df], axis=1)\n","test_df = pd.concat([test_df.drop(columns=categorical_cols), test_encoded_df], axis=1)\n","\n","# Update numerical columns after adding encoded columns\n","numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# Ensure 'price' and 'id' are not included in numerical columns for scaling\n","if 'price' in numerical_cols:\n","    numerical_cols.remove('price')\n","if 'id' in numerical_cols:\n","    numerical_cols.remove('id')\n","\n","# Apply log transformation to the target variable\n","train_df['price'] = np.log1p(train_df['price'])\n","\n","# Feature Scaling\n","scaler = StandardScaler()\n","train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n","test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n","\n","# Split the training data into features and target variable\n","X = train_df.drop(columns=['price'])\n","y = train_df['price']\n","\n","# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Use a smaller subset of the data for initial hyperparameter tuning\n","X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n","\n","# Hyperparameter Tuning with RandomizedSearchCV\n","param_dist = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 6, 9],\n","    'learning_rate': [0.01, 0.1, 0.3],\n","}\n","\n","model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', use_label_encoder=False)\n","\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='neg_mean_squared_error', cv=3, n_iter=10, verbose=1, n_jobs=-1, random_state=42)\n","random_search.fit(X_train_small, y_train_small)\n","\n","best_params = random_search.best_params_\n","print(\"Best parameters:\", best_params)\n","\n","# Train model with best parameters on full dataset\n","model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', tree_method='hist', use_label_encoder=False)\n","model.fit(X_train, y_train)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_val)\n","rmse = mean_squared_error(y_val, y_pred, squared=False)  # squared=False to get RMSE directly\n","print(f'Validation RMSE: {rmse}')\n","\n","# Cross-validation\n","cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","cv_rmse = np.sqrt(-cv_scores)\n","print(f'Cross-validated RMSE: {cv_rmse.mean()}')\n","\n","# Make predictions on the test data\n","predictions = model.predict(test_df)  # Drop 'id' column for prediction\n","predictions = np.expm1(predictions)  # Inverse log transformation to get back to original scale\n","\n","submission_df = pd.DataFrame({\n","    'id': test_ids,  # Use the stored 'id' column\n","    'price': predictions\n","})\n","\n","# Create submission file\n","submission_df.to_csv('submission.csv', index=False)\n","\n","print('Submission file created: submission.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAJ0Brf4MotU","executionInfo":{"status":"ok","timestamp":1717219057169,"user_tz":-330,"elapsed":215598,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"907a6d22-406e-40e8-95bf-f0c25d62f736"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 10 candidates, totalling 30 fits\n","Best parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1}\n","Validation RMSE: 0.496141781360692\n","Cross-validated RMSE: 0.5068145439920554\n","Submission file created: submission.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7IT7mAL2zcSE","executionInfo":{"status":"aborted","timestamp":1717218788294,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files.download('/content/submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"eBvGG9pl-S8Y","executionInfo":{"status":"ok","timestamp":1717219092041,"user_tz":-330,"elapsed":971,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"5ef0ffcd-3a2f-46bb-81fc-b0e80cde57be"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_44b420b9-e15d-4345-95b5-6df4533e003d\", \"submission.csv\", 564637)"]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n"],"metadata":{"id":"QPTSnie637_C","executionInfo":{"status":"aborted","timestamp":1717218788295,"user_tz":-330,"elapsed":7,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Measure skewness\n","skewness = prices.skew()\n","print(f'Skewness of the target variable (price): {skewness}')\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n","\n","# Display the statistics for the boxplot\n","Q1 = prices.quantile(0.25)\n","Q3 = prices.quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(f'Q1 (25th percentile): {Q1}')\n","print(f'Q3 (75th percentile): {Q3}')\n","print(f'IQR (Interquartile Range): {IQR}')\n","print(f'Lower bound for outliers: {lower_bound}')\n","print(f'Upper bound for outliers: {upper_bound}')\n","print(f'Number of outliers below lower bound: {(prices < lower_bound).sum()}')\n","print(f'Number of outliers above upper bound: {(prices > upper_bound).sum()}')\n"],"metadata":{"id":"5LFPqi545RVU","executionInfo":{"status":"aborted","timestamp":1717218788295,"user_tz":-330,"elapsed":7,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Measure skewness\n","skewness = prices.skew()\n","print(f'Skewness of the target variable (price): {skewness}')\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n","\n","# Display the statistics for the boxplot\n","Q1 = prices.quantile(0.25)\n","Q3 = prices.quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(f'Q1 (25th percentile): {Q1}')\n","print(f'Q3 (75th percentile): {Q3}')\n","print(f'IQR (Interquartile Range): {IQR}')\n","print(f'Lower bound for outliers: {lower_bound}')\n","print(f'Upper bound for outliers: {upper_bound}')\n","print(f'Number of outliers below lower bound: {(prices < lower_bound).sum()}')\n","print(f'Number of outliers above upper bound: {(prices > upper_bound).sum()}')\n","\n","# Apply log transformation to reduce skewness\n","train_df['price'] = np.log1p(train_df['price'])\n","\n","# Ensure to also apply the transformation to the validation set if needed\n","# If 'price' column in validation set (if any)\n","# validation_df['price'] = np.log1p(validation_df['price'])\n","\n","# Check the new distribution after log transformation\n","plt.figure(figsize=(10, 6))\n","sns.histplot(train_df['price'], bins=50, kde=True)\n","plt.title('Distribution of Log-Transformed Prices')\n","plt.xlabel('Log-Transformed Price')\n","plt.ylabel('Frequency')\n","plt.show()\n"],"metadata":{"id":"03TIza_Z6CwG","executionInfo":{"status":"aborted","timestamp":1717218788295,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5vzDzrBRv2E","executionInfo":{"status":"ok","timestamp":1717219829951,"user_tz":-330,"elapsed":991,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"6e3420ec-33bb-406a-d519-630069813869"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["# Replace with your GitHub email and username\n","!git config --global user.email \"deepanshanmugam13@gmail.com\"\n","!git config --global user.name \"Dheep13\"\n"],"metadata":{"id":"JuS-V518SYnx","executionInfo":{"status":"ok","timestamp":1717220167102,"user_tz":-330,"elapsed":518,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/Dheep13/KaggleContest.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWJuAEQXS199","executionInfo":{"status":"ok","timestamp":1717220118943,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"cf9b36fc-6ec1-4778-f926-4c58de8bf30d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'KaggleContest' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["import os\n","\n","dataset_dir = '/content'\n","for dirname, _, filenames in os.walk(dataset_dir):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B9CUs5TTGXH","executionInfo":{"status":"ok","timestamp":1717220220637,"user_tz":-330,"elapsed":739,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"ef05902b-8597-47c4-ebca-1d487730f6a7"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/train.csv\n","/content/kagglex-cohort4.zip\n","/content/submission.csv\n","/content/test.csv\n","/content/kaggle.json\n","/content/sample_submission.csv\n","/content/.config/gce\n","/content/.config/config_sentinel\n","/content/.config/default_configs.db\n","/content/.config/.last_update_check.json\n","/content/.config/.last_opt_in_prompt.yaml\n","/content/.config/active_config\n","/content/.config/.last_survey_prompt.yaml\n","/content/.config/logs/2024.05.30/13.23.26.998698.log\n","/content/.config/logs/2024.05.30/13.36.15.385607.log\n","/content/.config/logs/2024.05.30/13.30.33.059431.log\n","/content/.config/logs/2024.05.30/13.30.21.274210.log\n","/content/.config/logs/2024.05.30/13.36.16.059291.log\n","/content/.config/logs/2024.05.30/13.36.03.155708.log\n","/content/.config/configurations/config_default\n","/content/KaggleContest/README.md\n","/content/KaggleContest/assignments/RAG_Test_Gamma.ipynb\n","/content/KaggleContest/assignments/Multiclass_Classification-deepan.ipynb\n","/content/KaggleContest/assignments/BirdClassification using Transformers-final-deepan.ipynb\n","/content/KaggleContest/assignments/Linear_Regression-deepan.ipynb\n","/content/KaggleContest/assignments/binary-classification-competition-deepan.ipynb\n","/content/KaggleContest/assignments/Artificial Neural Networks Competition-deepan.ipynb\n","/content/KaggleContest/assignments/Copy of Regression with ANNs-deepan.ipynb\n","/content/KaggleContest/.git/config\n","/content/KaggleContest/.git/HEAD\n","/content/KaggleContest/.git/description\n","/content/KaggleContest/.git/index\n","/content/KaggleContest/.git/packed-refs\n","/content/KaggleContest/.git/logs/HEAD\n","/content/KaggleContest/.git/logs/refs/heads/main\n","/content/KaggleContest/.git/logs/refs/remotes/origin/HEAD\n","/content/KaggleContest/.git/objects/pack/pack-0e9cc269a66cc86d5fd62f351a59fca0cfaeda84.idx\n","/content/KaggleContest/.git/objects/pack/pack-0e9cc269a66cc86d5fd62f351a59fca0cfaeda84.pack\n","/content/KaggleContest/.git/hooks/pre-merge-commit.sample\n","/content/KaggleContest/.git/hooks/pre-receive.sample\n","/content/KaggleContest/.git/hooks/pre-rebase.sample\n","/content/KaggleContest/.git/hooks/applypatch-msg.sample\n","/content/KaggleContest/.git/hooks/prepare-commit-msg.sample\n","/content/KaggleContest/.git/hooks/push-to-checkout.sample\n","/content/KaggleContest/.git/hooks/pre-push.sample\n","/content/KaggleContest/.git/hooks/pre-applypatch.sample\n","/content/KaggleContest/.git/hooks/commit-msg.sample\n","/content/KaggleContest/.git/hooks/pre-commit.sample\n","/content/KaggleContest/.git/hooks/post-update.sample\n","/content/KaggleContest/.git/hooks/fsmonitor-watchman.sample\n","/content/KaggleContest/.git/hooks/update.sample\n","/content/KaggleContest/.git/info/exclude\n","/content/KaggleContest/.git/refs/heads/main\n","/content/KaggleContest/.git/refs/remotes/origin/HEAD\n","/content/sample_data/anscombe.json\n","/content/sample_data/README.md\n","/content/sample_data/california_housing_test.csv\n","/content/sample_data/california_housing_train.csv\n","/content/sample_data/mnist_test.csv\n","/content/sample_data/mnist_train_small.csv\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Configure Git\n","!git config --global user.email \"deepanshanmugam13@gmail.com\"\n","!git config --global user.name \"Dheep13\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ew95ltc_Tsna","executionInfo":{"status":"ok","timestamp":1717220905073,"user_tz":-330,"elapsed":35408,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"b37d01cb-eb6d-4447-b5f8-da4e47566a41"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Clone your repository\n","!git clone https://github.com/Dheep13/KaggleContest.git\n","\n","# Path to the notebook in Google Drive\n","notebook_path = '/content/drive/My Drive/Colab Notebooks/KaggleX_Ver2.ipynb'\n","\n","# Copy the notebook file to the repository directory\n","!cp \"{notebook_path}\" \"/content/KaggleContest/\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InK2MZs_QbCA","executionInfo":{"status":"ok","timestamp":1717220916540,"user_tz":-330,"elapsed":935,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"603c6d8c-a62e-482c-a996-ac36cdbe4c52"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'KaggleContest' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["# Change directory to your repository\n","%cd /content/KaggleContest\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7CRyAJTQfe7","executionInfo":{"status":"ok","timestamp":1717219872432,"user_tz":-330,"elapsed":584,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"e3535ae3-366e-4987-e0f4-181896769f2a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","source":["# Add the notebook file to the staging area\n","!git add KaggleX_Ver2.ipynb\n","\n"],"metadata":{"id":"_U-dOD56QhbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Commit the changes\n","!git commit -m \"Add notebook file from Colab\"\n","\n"],"metadata":{"id":"VvaGMw7PVnU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Push the changes to the remote repository\n","# Replace 'https://yourusername:yourtoken@github.com/Dheep13/KaggleContest.git' with your actual repository URL\n","!git push https://yourusername:yourtoken@github.com/Dheep13/KaggleContest.git\n"],"metadata":{"id":"1EAddLdwVquP"},"execution_count":null,"outputs":[]}]}