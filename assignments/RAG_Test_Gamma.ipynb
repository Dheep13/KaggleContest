{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8087817,"sourceType":"datasetVersion","datasetId":4774436}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade huggingface_hub\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install -q transformers\n!pip install -q chromadb\n!pip install -q sentence_transformers\n!pip install -q torch\n!pip install -q tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T05:28:29.482776Z","iopub.execute_input":"2024-04-11T05:28:29.483147Z","iopub.status.idle":"2024-04-11T05:30:35.985134Z","shell.execute_reply.started":"2024-04-11T05:28:29.483120Z","shell.execute_reply":"2024-04-11T05:30:35.984039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom transformers import pipeline, PretrainedConfig,AutoTokenizer, AutoModelForCausalLM\nfrom IPython.display import display, Markdown\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:30:35.987763Z","iopub.execute_input":"2024-04-11T05:30:35.988571Z","iopub.status.idle":"2024-04-11T05:30:55.053090Z","shell.execute_reply.started":"2024-04-11T05:30:35.988534Z","shell.execute_reply":"2024-04-11T05:30:55.052293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading data\ndatafile = pd.read_csv(\"/kaggle/input/rag-dataset/kaggle_winning_solutions_methods.csv\")\ndatafile.info()\n\ndatafile.head()\n\n# Data cleaning\ndatafile = datafile[['place','competition_name','metric','year','writeup','methods']] # omit not required columns\ndatafile.drop_duplicates(inplace=True) # remove duplicate rows\ndatafile.info() \n\n\ndatafile['writeup'] = datafile['writeup'].apply(lambda x: BeautifulSoup(x,'html.parser').get_text())\ndatafile.reset_index()\ndatafile.head()\n\n\n# File structure for indexing in vector database\ntotal_entries = len(datafile) # total number of data points\n\n# Document for writeup\ndocuments = [f\"{datafile.iloc[i]['place']} place solution :  \\n competition name : {datafile.iloc[i]['competition_name']} \\n metric : {datafile.iloc[i]['metric']} \\n year : {datafile.iloc[i]['year']} \\n writeup : {datafile.iloc[i]['writeup']} \\n methods : {datafile.iloc[i]['methods']}\" for i in range(total_entries)]\n\n# Metadata to save for each document\nmetadatas =  [\n            {\n            'place':str(datafile['place'].iloc[j]),\n            'competition_name':str(datafile['competition_name'].iloc[j]),\n            'year':str(datafile['year'].iloc[j])\n            } for j in range(total_entries)\n            ]\n\n\n# Configure vector database\nclient = chromadb.Client() # initiate client\nef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name = 'all-MiniLM-L6-v2') # Embedding function for documents\n# Create a vector database\ncollection = client.create_collection(\n    name = 'kaggle_competition_solutions',\n    embedding_function = ef ,\n    metadata={\"hnsw:space\": \"l2\"}\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:30:55.054240Z","iopub.execute_input":"2024-04-11T05:30:55.055168Z","iopub.status.idle":"2024-04-11T05:31:04.815208Z","shell.execute_reply.started":"2024-04-11T05:30:55.055142Z","shell.execute_reply":"2024-04-11T05:31:04.814460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ingest documnets into vector database\ncollection.add(\n    documents = documents,\n    ids = [str(i) for i in range(total_entries)],\n    metadatas = metadatas\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:31:04.816190Z","iopub.execute_input":"2024-04-11T05:31:04.816458Z","iopub.status.idle":"2024-04-11T05:32:40.385626Z","shell.execute_reply.started":"2024-04-11T05:31:04.816436Z","shell.execute_reply":"2024-04-11T05:32:40.384662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup the environment\n\n\naccess_token_read = UserSecretsClient().get_secret(\"HUUGGINGFACE_TOKEN\")\nlogin(token = access_token_read)\n\n# Load the model\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",device_map=\"cuda\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-11T05:32:40.388120Z","iopub.execute_input":"2024-04-11T05:32:40.388501Z","iopub.status.idle":"2024-04-11T05:33:15.899188Z","shell.execute_reply.started":"2024-04-11T05:32:40.388467Z","shell.execute_reply":"2024-04-11T05:33:15.898297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(prompt):\n    input_ids = tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU\n    output = model.generate(input_ids, max_length=4056)  # Adjust generation parameters as needed\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    return generated_text\n\ndef query_database(user_prompt):\n    output = collection.query(\n    query_texts=user_prompt,\n    n_results=1\n    )\n    return output['documents'][0][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:53:47.735024Z","iopub.execute_input":"2024-04-11T05:53:47.735473Z","iopub.status.idle":"2024-04-11T05:53:47.741947Z","shell.execute_reply.started":"2024-04-11T05:53:47.735440Z","shell.execute_reply":"2024-04-11T05:53:47.740886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_prompt = 'Explain the 12 place solution of Signal Search competition'\n\nwriteup = query_database(user_prompt) # vector database output\n\nprompt_template = [\n    { \"role\": \"user\", \n     \"content\": f\"You are teacher, explain the given below content based on the instructions. Generate a response in simple to understand words ### instruction {user_prompt}  ### content {writeup}\" \n    },\n]\n\n# User prompt and writeup in prompt template format\nprompt = tokenizer.apply_chat_template(prompt_template, tokenize=False, add_generation_prompt=True)\n\n# Generate respose\noutputs = generate_text(prompt)\ndisplay(Markdown(outputs[len(prompt)-2:]))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:54:02.974165Z","iopub.execute_input":"2024-04-11T05:54:02.974555Z","iopub.status.idle":"2024-04-11T05:54:13.376916Z","shell.execute_reply.started":"2024-04-11T05:54:02.974526Z","shell.execute_reply":"2024-04-11T05:54:13.375997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Configuration\nconfig = PretrainedConfig(\n    do_sample=True,\n    temperature=0.1,\n    top_k=30,\n    top_p=0.7,    \n    torch_dtype=torch.bfloat16, \n    )\n\n# Pipeline\npipe = pipeline(\n    task = 'text-generation',\n    model = model,\n    tokenizer = tokenizer,\n    max_new_tokens = 4056,\n    device_map=\"auto\",\n    config = config\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:58:00.562687Z","iopub.execute_input":"2024-04-11T05:58:00.563499Z","iopub.status.idle":"2024-04-11T05:58:00.569924Z","shell.execute_reply.started":"2024-04-11T05:58:00.563463Z","shell.execute_reply":"2024-04-11T05:58:00.568888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Gemma_generate(user_prompt):\n    writeup = query_database(user_prompt)\n\n    prompt = [\n        { \"role\": \"user\", \n         \"content\": f\"You are teacher, explain the given below content based on the instructions. Generate a response in simple to understand words ### instruction {user_prompt}  ### content {writeup}\" \n        },\n    ]\n\n    output = pipe(\n        prompt\n        )\n    return output[0]['generated_text'][1]['content']","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:58:12.919777Z","iopub.execute_input":"2024-04-11T05:58:12.920657Z","iopub.status.idle":"2024-04-11T05:58:12.926131Z","shell.execute_reply.started":"2024-04-11T05:58:12.920622Z","shell.execute_reply":"2024-04-11T05:58:12.925134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_prompt = 'Explain the 12 place solution of Signal Search competition'  \nresponse = Gemma_generate(user_prompt)\ndisplay(Markdown(response))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T05:58:23.594269Z","iopub.execute_input":"2024-04-11T05:58:23.595192Z","iopub.status.idle":"2024-04-11T05:58:33.306176Z","shell.execute_reply.started":"2024-04-11T05:58:23.595157Z","shell.execute_reply":"2024-04-11T05:58:33.305212Z"},"trusted":true},"execution_count":null,"outputs":[]}]}