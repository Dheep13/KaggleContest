{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"elapsed":30773,"status":"ok","timestamp":1708010249186,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"4IzBlnSmVjom","outputId":"0d91ec60-1148-4a32-81f6-81216ae81511"},"outputs":[],"source":["from google.colab import files\n","\n","# This will prompt you to select the kaggle.json file\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":606,"status":"ok","timestamp":1708010272263,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"19997eJwWDF3"},"outputs":[],"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1477,"status":"ok","timestamp":1708010303509,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"uf3jptPkWK1Y","outputId":"0b8b6a3c-adbe-4a8a-9729-fc6f610bf925"},"outputs":[],"source":["!kaggle competitions download -c copy-of-regression-with-anns"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708010342409,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"z3YgnxRyWShm"},"outputs":[],"source":["!unzip -q copy-of-regression-with-anns.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1708011049353,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"EfUpfSACYQrV","outputId":"571e78c1-0ce2-4f3d-e3c4-722ba5444ec8"},"outputs":[],"source":["!ls\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":583,"status":"ok","timestamp":1708014722795,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"q-k1gQ1xY3pK"},"outputs":[],"source":["# from zipfile import ZipFile\n","# import os\n","import pandas as pd\n","import os\n","\n","train_df = pd.read_csv('/content/train.csv')\n","test_df = pd.read_csv('/content/test.csv')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":1649,"status":"ok","timestamp":1708015779136,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"_shPEXHSYVIb","outputId":"8e78df1d-5835-40a1-fa8f-a2cf0e352ec9"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","test_df.rename(columns={'Unnamed: 0': 'ID'}, inplace=True)\n","\n","# Now, you can drop 'Unnamed: 0' from train and validation datasets if you haven't already\n","train_df.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')\n","\n","# Identify numerical and categorical features\n","numerical_features = ['SO2', 'NO2', 'CO', 'O3', 'Temp', 'Press', 'DewP', 'Rain', 'WinSpeed']\n","categorical_features = ['WinDir', 'Station']  # Assuming 'WinDir' and 'Station' are categorical\n","\n","# Define the ColumnTransformer to apply different preprocessing to different columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_features),\n","        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n","    ])\n","\n","# Split the train dataset into X (features) and y (target)\n","X_train_full = train_df.drop('PM2.5', axis=1)\n","y_train_full = train_df['PM2.5']\n","\n","# Further split the full training data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n","\n","# Apply preprocessing\n","X_train_preprocessed = preprocessor.fit_transform(X_train)\n","X_val_preprocessed = preprocessor.transform(X_val)\n","X_test_preprocessed = preprocessor.transform(test_df)\n","\n","# Display the shape of the processed feature sets to confirm\n","X_train_preprocessed.shape, X_val_preprocessed.shape, X_test_preprocessed.shape\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Assuming `train_df` is your DataFrame\n","# Visualizing outliers for 'Temp' feature using a boxplot\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=train_df['Temp'])\n","plt.title('Boxplot of Temperature (Temp)')\n","plt.show()\n","\n","# Function to remove outliers based on the IQR method for all numerical features\n","def remove_outliers(df, features):\n","    for feature in features:\n","        Q1 = df[feature].quantile(0.25)\n","        Q3 = df[feature].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        # Filtering out outliers\n","        df = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n","    return df\n","\n","# Removing outliers from the train_df\n","train_df_filtered = remove_outliers(train_df, numerical_features)\n","\n","# You might want to reset index after removing outliers\n","train_df_filtered.reset_index(drop=True, inplace=True)\n","\n","\n","# Split the filtered train dataset into X (features) and y (target)\n","X_train_full_filtered = train_df_filtered.drop('PM2.5', axis=1)\n","y_train_full_filtered = train_df_filtered['PM2.5']\n","\n","# Further split the full training data into training and validation sets\n","X_train_filtered, X_val_filtered, y_train_filtered, y_val_filtered = train_test_split(X_train_full_filtered, y_train_full_filtered, test_size=0.2, random_state=42)\n","\n","# Apply preprocessing on the filtered dataset\n","X_train_preprocessed_filtered = preprocessor.fit_transform(X_train_filtered)\n","X_val_preprocessed_filtered = preprocessor.transform(X_val_filtered)\n","\n","# Display the shape of the processed feature sets to confirm\n","print(X_train_preprocessed_filtered.shape, X_val_preprocessed_filtered.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":366,"status":"ok","timestamp":1708015807932,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"Db_Y0ScWaCky"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","\n","# Convert the sparse matrix to a dense matrix, then to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_preprocessed.toarray(), dtype=torch.float32) if hasattr(X_train_preprocessed, 'toarray') else torch.tensor(X_train_preprocessed, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n","X_val_tensor = torch.tensor(X_val_preprocessed.toarray(), dtype=torch.float32) if hasattr(X_val_preprocessed, 'toarray') else torch.tensor(X_val_preprocessed, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n","X_test_tensor = torch.tensor(X_test_preprocessed.toarray(), dtype=torch.float32) if hasattr(X_test_preprocessed, 'toarray') else torch.tensor(X_test_preprocessed, dtype=torch.float32)\n","\n","\n","# Create TensorDatasets for training and validation sets\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","\n","# Create DataLoaders for training and validation sets\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1708015818495,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"XZ_PuOBHaQ2V"},"outputs":[],"source":["#define the ANN model architecture\n","# class RegressionANN(nn.Module):\n","#     def __init__(self, input_size):\n","#         super(RegressionANN, self).__init__()\n","#         self.layer1 = nn.Linear(input_size, 128)\n","#         self.layer2 = nn.Linear(128, 64)\n","#         self.output_layer = nn.Linear(64, 1)\n","#         self.relu = nn.ReLU()\n","\n","#     def forward(self, x):\n","#         x = self.relu(self.layer1(x))\n","#         x = self.relu(self.layer2(x))\n","#         x = self.output_layer(x)\n","#         return x\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class EnhancedRegressionANN(nn.Module):\n","    def __init__(self, input_size):\n","        super(EnhancedRegressionANN, self).__init__()\n","        # Increase the depth and adjust the size of each layer\n","        self.layer1 = nn.Linear(input_size, 256)\n","        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization layer\n","        self.dropout1 = nn.Dropout(0.1)  # Dropout layer\n","\n","        self.layer2 = nn.Linear(256, 128)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.dropout2 = nn.Dropout(0.1)\n","\n","        self.layer3 = nn.Linear(128, 64)\n","        self.bn3 = nn.BatchNorm1d(64)\n","        self.dropout3 = nn.Dropout(0.1)\n","\n","        self.layer4 = nn.Linear(64, 32)\n","        self.bn4 = nn.BatchNorm1d(32)\n","        self.dropout4 = nn.Dropout(0.1)\n","\n","        self.output_layer = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.layer1(x)))\n","        x = self.dropout1(x)\n","        x = F.relu(self.bn2(self.layer2(x)))\n","        x = self.dropout2(x)\n","        x = F.relu(self.bn3(self.layer3(x)))\n","        x = self.dropout3(x)\n","        x = F.relu(self.bn4(self.layer4(x)))\n","        x = self.dropout4(x)\n","        x = self.output_layer(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1708015821527,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"soGz729KaV9c"},"outputs":[],"source":["# Initialize the ANN model\n","input_size = X_train_tensor.shape[1]\n","# model = RegressionANN(input_size)\n","model = EnhancedRegressionANN(input_size)\n","\n","# Define the loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.003)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210964,"status":"ok","timestamp":1708016034333,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"HO1ycxtnakgd","outputId":"714bbaf7-24c3-451d-f5cc-6344c2ce9b88"},"outputs":[],"source":["epochs = 20\n","for epoch in range(epochs):\n","    model.train()\n","    for inputs, targets in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss.item()\n","    val_loss /= len(val_loader)\n","\n","    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":801,"status":"ok","timestamp":1708016039292,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"TyXMwGHCapGt","outputId":"503c3ea8-ab68-43fb-821e-7aaba68d6012"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from math import sqrt\n","import numpy as np\n","\n","# Ensure model is in evaluation mode\n","model.eval()\n","\n","# Collect all predictions and actual values\n","all_predictions = []\n","all_actuals = []\n","\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        outputs = model(inputs)\n","        all_predictions.extend(outputs.view(-1).tolist())\n","        all_actuals.extend(targets.view(-1).tolist())\n","\n","# Convert lists to arrays for calculation\n","all_predictions = np.array(all_predictions)\n","all_actuals = np.array(all_actuals)\n","\n","# Calculate MAE and RMSE\n","mae = mean_absolute_error(all_actuals, all_predictions)\n","rmse = sqrt(mean_squared_error(all_actuals, all_predictions))\n","\n","print(f'Mean Absolute Error (MAE): {mae}')\n","print(f'Root Mean Square Error (RMSE): {rmse}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"elapsed":4188,"status":"ok","timestamp":1708016678602,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"},"user_tz":-330},"id":"O1uqNW8EasMO","outputId":"2d76423c-d2e7-43fc-8020-3d3429a1a976"},"outputs":[],"source":["from google.colab import drive\n","\n","test_predictions = []\n","with torch.no_grad():\n","    test_predictions = model(X_test_tensor).view(-1).numpy()\n","\n","# Assuming `test_predictions` contains the predictions for the test dataset\n","# and `test_df` is your test DataFrame which should have the 'ID' column\n","\n","print(test_df.head())\n","\n","# Generate IDs based on the length of your predictions\n","IDs = np.arange(len(test_predictions))\n","\n","# Create a DataFrame for the submission\n","submission_df = pd.DataFrame({\n","    # 'ID': test_df.iloc[:, 0],  # Make sure your test DataFrame has an 'ID' column\n","    'ID': IDs,\n","    'PM2.5': test_predictions.flatten()  # Flatten the predictions if necessary\n","})\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","# Save the DataFrame to a .csv file for submission\n","submission_file_path = '/content/drive/My Drive/submission_ann_regression.csv'\n","submission_df.to_csv(submission_file_path, index=False)\n","\n","submission_df.head()\n","\n","# Prepare your submission based on the test predictions\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOaxrL6u40v+Wl0FSnnQ/dY","provenance":[{"file_id":"15lOvlJPKxUbiudQnILBZ_zTdX8D9-a1B","timestamp":1708017067190}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
