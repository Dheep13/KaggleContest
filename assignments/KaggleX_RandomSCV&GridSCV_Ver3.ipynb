{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19w060kHYvdVHZJxIrpWsaXd963CgU1j6","timestamp":1717223338108}],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP1+taUtMqW/xV24Y3q7yXN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"7new4wl_vmpH","executionInfo":{"status":"ok","timestamp":1717223545821,"user_tz":-330,"elapsed":10610,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"a2fad850-d4e9-453c-f7d8-5b928b385ec3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5d1e8574-a2c1-4380-9d45-d86a75789993\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5d1e8574-a2c1-4380-9d45-d86a75789993\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"deepanshanmugam\",\"key\":\"b6ba4dd7b52d960f39aff9202855df43\"}'}"]},"metadata":{},"execution_count":1}],"source":["from google.colab import files\n","\n","# This will prompt you to select the kaggle.json file\n","files.upload()\n"]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n"],"metadata":{"id":"1K93XjvTwEpP","executionInfo":{"status":"ok","timestamp":1717223546659,"user_tz":-330,"elapsed":841,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# !pip install kaggle\n","!kaggle competitions download -c kagglex-cohort4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cbjT1IxwI1z","executionInfo":{"status":"ok","timestamp":1717223548504,"user_tz":-330,"elapsed":1851,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"c27c0aeb-06c5-4c18-cf19-823f1673f332"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading kagglex-cohort4.zip to /content\n"," 92% 2.00M/2.17M [00:00<00:00, 3.07MB/s]\n","100% 2.17M/2.17M [00:00<00:00, 2.90MB/s]\n"]}]},{"cell_type":"code","source":["!unzip -q kagglex-cohort4.zip\n"],"metadata":{"id":"w86ccyl0wL5H","executionInfo":{"status":"ok","timestamp":1717223548504,"user_tz":-330,"elapsed":11,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# View the files in the dataset directory\n","import os\n","\n","dataset_dir = '/content'\n","for dirname, _, filenames in os.walk(dataset_dir):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QupkKRNqwPuq","executionInfo":{"status":"ok","timestamp":1717223548504,"user_tz":-330,"elapsed":10,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"f0e509dd-c94a-4c0b-e206-48e618bfa483"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/train.csv\n","/content/kagglex-cohort4.zip\n","/content/test.csv\n","/content/kaggle.json\n","/content/sample_submission.csv\n","/content/.config/gce\n","/content/.config/config_sentinel\n","/content/.config/default_configs.db\n","/content/.config/.last_update_check.json\n","/content/.config/.last_opt_in_prompt.yaml\n","/content/.config/active_config\n","/content/.config/.last_survey_prompt.yaml\n","/content/.config/logs/2024.05.30/13.23.26.998698.log\n","/content/.config/logs/2024.05.30/13.36.15.385607.log\n","/content/.config/logs/2024.05.30/13.30.33.059431.log\n","/content/.config/logs/2024.05.30/13.30.21.274210.log\n","/content/.config/logs/2024.05.30/13.36.16.059291.log\n","/content/.config/logs/2024.05.30/13.36.03.155708.log\n","/content/.config/configurations/config_default\n","/content/sample_data/anscombe.json\n","/content/sample_data/README.md\n","/content/sample_data/california_housing_test.csv\n","/content/sample_data/california_housing_train.csv\n","/content/sample_data/mnist_test.csv\n","/content/sample_data/mnist_train_small.csv\n"]}]},{"cell_type":"code","source":["\n","###Version 1 - scaling and preprocessing done for price and id\n","# import xgboost as xgb\n","# from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n","# from sklearn.metrics import mean_squared_error\n","# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","# import pandas as pd\n","# import numpy as np\n","\n","# # Load datasets\n","# train_df = pd.read_csv('/content/train.csv')\n","# test_df = pd.read_csv('/content/test.csv')\n","\n","# # Feature Engineering\n","# # Example of creating new features\n","# train_df['age'] = 2024 - train_df['model_year']  # Assuming the year column exists and 2024 is the current year\n","# test_df['age'] = 2024 - test_df['model_year']\n","\n","# # Handle missing values for numerical columns\n","# train_df.fillna(train_df.select_dtypes(include=['number']).mean(), inplace=True)\n","# test_df.fillna(test_df.select_dtypes(include=['number']).mean(), inplace=True)\n","\n","# # Ensure 'age' column is included in numerical columns\n","# numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# # Remove redundant or unimportant features\n","# # Drop columns with low variance (only on numerical columns)\n","# low_variance_cols = train_df[numerical_cols].var()[train_df[numerical_cols].var() < 0.1].index.tolist()\n","# train_df.drop(columns=low_variance_cols, inplace=True)\n","# test_df.drop(columns=low_variance_cols, inplace=True)\n","\n","# # Update numerical columns after dropping low variance columns\n","# numerical_cols = [col for col in numerical_cols if col not in low_variance_cols]\n","\n","# # Drop highly correlated features (only on numerical columns)\n","# correlation_matrix = train_df[numerical_cols].corr().abs()\n","# upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n","# high_correlation_cols = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n","# train_df.drop(columns=high_correlation_cols, inplace=True)\n","# test_df.drop(columns=high_correlation_cols, inplace=True)\n","\n","# # Update numerical columns after dropping highly correlated columns\n","# numerical_cols = [col for col in numerical_cols if col not in high_correlation_cols]\n","\n","# # Drop the 'clean_title' column\n","# train_df.drop(columns=['clean_title'], inplace=True)\n","# test_df.drop(columns=['clean_title'], inplace=True)\n","\n","# # Transform existing features\n","# # Log transform skewed features (only on numerical columns, excluding 'price')\n","# skewed_cols = train_df[numerical_cols].skew().abs() > 0.75\n","# skewed_features = skewed_cols[skewed_cols].index.tolist()\n","# if 'price' in skewed_features:\n","#     skewed_features.remove('price')  # Ensure 'price' is not included\n","# train_df[skewed_features] = np.log1p(train_df[skewed_features])\n","# test_df[skewed_features] = np.log1p(test_df[skewed_features])\n","\n","# # Identify categorical columns\n","# categorical_cols = train_df.select_dtypes(include=['object']).columns\n","\n","# # Apply OneHotEncoder to categorical columns\n","# ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","# train_encoded = ohe.fit_transform(train_df[categorical_cols])\n","# test_encoded = ohe.transform(test_df[categorical_cols])\n","\n","# # Convert encoded data to DataFrames\n","# train_encoded_df = pd.DataFrame(train_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","# test_encoded_df = pd.DataFrame(test_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","\n","# # Concatenate encoded columns back to the original DataFrames\n","# train_df = pd.concat([train_df.drop(columns=categorical_cols), train_encoded_df], axis=1)\n","# test_df = pd.concat([test_df.drop(columns=categorical_cols), test_encoded_df], axis=1)\n","\n","# # Update numerical columns after adding encoded columns\n","# numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# # Ensure 'price' is not included in numerical columns for scaling\n","# if 'price' in numerical_cols:\n","#     numerical_cols.remove('price')\n","\n","# # Apply log transformation to the target variable\n","# train_df['price'] = np.log1p(train_df['price'])\n","\n","# upper_bound = 95000  # Calculated upper bound\n","# train_df['price'] = train_df['price'].clip(upper=upper_bound)\n","\n","\n","# # Feature Scaling\n","# scaler = StandardScaler()\n","# train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n","# test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n","\n","# # Split the training data into features and target variable\n","# X = train_df.drop(columns=['price'])\n","# y = train_df['price']\n","\n","# # Split data into training and validation sets\n","# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# # Use a smaller subset of the data for initial hyperparameter tuning\n","# X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n","\n","# # Hyperparameter Tuning with RandomizedSearchCV\n","# param_dist = {\n","#     'n_estimators': [100, 200, 300],\n","#     'max_depth': [3, 6, 9],\n","#     'learning_rate': [0.01, 0.1, 0.3],\n","# }\n","\n","# model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', device='cuda')\n","\n","# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring='neg_mean_squared_error', cv=3, n_iter=10, verbose=1, n_jobs=-1, random_state=42)\n","# random_search.fit(X_train_small, y_train_small)\n","\n","# best_params = random_search.best_params_\n","# print(\"Best parameters:\", best_params)\n","\n","# # Train model with best parameters on full dataset\n","# model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', tree_method='hist', device='cuda')\n","# model.fit(X_train, y_train)\n","\n","# # Evaluate the model\n","# y_pred = model.predict(X_val)\n","# rmse = mean_squared_error(y_val, y_pred, squared=False)  # squared=False to get RMSE directly\n","# print(f'Validation RMSE: {rmse}')\n","\n","# # Cross-validation\n","# cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","# cv_rmse = np.sqrt(-cv_scores)\n","# print(f'Cross-validated RMSE: {cv_rmse.mean()}')\n","\n"],"metadata":{"id":"E_FBuZo-vqV9","executionInfo":{"status":"ok","timestamp":1717223548504,"user_tz":-330,"elapsed":7,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","import pandas as pd\n","import numpy as np\n","\n","# Load datasets\n","train_df = pd.read_csv('/content/train.csv')\n","test_df = pd.read_csv('/content/test.csv')\n","\n","# Store the 'id' column from test data\n","test_ids = test_df['id']\n","\n","# Feature Engineering\n","# Example of creating new features\n","train_df['age'] = 2024 - train_df['model_year']  # Assuming the year column exists and 2024 is the current year\n","test_df['age'] = 2024 - test_df['model_year']\n","\n","# Handle missing values for numerical columns\n","train_df.fillna(train_df.select_dtypes(include=['number']).mean(), inplace=True)\n","test_df.fillna(test_df.select_dtypes(include=['number']).mean(), inplace=True)\n","\n","# Ensure 'age' column is included in numerical columns\n","numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# Remove redundant or unimportant features\n","# Drop columns with low variance (only on numerical columns)\n","low_variance_cols = train_df[numerical_cols].var()[train_df[numerical_cols].var() < 0.1].index.tolist()\n","train_df.drop(columns=low_variance_cols, inplace=True)\n","test_df.drop(columns=low_variance_cols, inplace=True)\n","\n","# Update numerical columns after dropping low variance columns\n","numerical_cols = [col for col in numerical_cols if col not in low_variance_cols]\n","\n","# Drop highly correlated features (only on numerical columns)\n","correlation_matrix = train_df[numerical_cols].corr().abs()\n","upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n","high_correlation_cols = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n","train_df.drop(columns=high_correlation_cols, inplace=True)\n","test_df.drop(columns=high_correlation_cols, inplace=True)\n","\n","# Update numerical columns after dropping highly correlated columns\n","numerical_cols = [col for col in numerical_cols if col not in high_correlation_cols]\n","\n","# Drop the 'clean_title' column\n","train_df.drop(columns=['clean_title'], inplace=True)\n","test_df.drop(columns=['clean_title'], inplace=True)\n","\n","# Transform existing features\n","# Log transform skewed features (only on numerical columns, excluding 'price')\n","skewed_cols = train_df[numerical_cols].skew().abs() > 0.75\n","skewed_features = skewed_cols[skewed_cols].index.tolist()\n","if 'price' in skewed_features:\n","    skewed_features.remove('price')  # Ensure 'price' is not included\n","train_df[skewed_features] = np.log1p(train_df[skewed_features])\n","test_df[skewed_features] = np.log1p(test_df[skewed_features])\n","\n","# Identify categorical columns\n","categorical_cols = train_df.select_dtypes(include=['object']).columns\n","\n","# Apply OneHotEncoder to categorical columns\n","ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","train_encoded = ohe.fit_transform(train_df[categorical_cols])\n","test_encoded = ohe.transform(test_df[categorical_cols])\n","\n","# Convert encoded data to DataFrames\n","train_encoded_df = pd.DataFrame(train_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","test_encoded_df = pd.DataFrame(test_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n","\n","# Concatenate encoded columns back to the original DataFrames\n","train_df = pd.concat([train_df.drop(columns=categorical_cols), train_encoded_df], axis=1)\n","test_df = pd.concat([test_df.drop(columns=categorical_cols), test_encoded_df], axis=1)\n","\n","# Update numerical columns after adding encoded columns\n","numerical_cols = train_df.select_dtypes(include=['number']).columns.tolist()\n","\n","# Ensure 'price' and 'id' are not included in numerical columns for scaling\n","if 'price' in numerical_cols:\n","    numerical_cols.remove('price')\n","if 'id' in numerical_cols:\n","    numerical_cols.remove('id')\n","\n","# Apply log transformation to the target variable\n","train_df['price'] = np.log1p(train_df['price'])\n","\n","# Feature Scaling\n","scaler = StandardScaler()\n","train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n","test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n","\n","# Split the training data into features and target variable\n","X = train_df.drop(columns=['price'])\n","y = train_df['price']\n","\n","# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Use a smaller subset of the data for initial hyperparameter tuning\n","X_train_small, _, y_train_small, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n","\n","# RandomizedSearchCV\n","param_dist = {\n","    'n_estimators': [100, 200, 300, 400, 500],\n","    'max_depth': [3, 6, 9, 12],\n","    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n","    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n","}\n","\n","model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist',device='cuda', use_label_encoder=False)\n","\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1, random_state=42)\n","random_search.fit(X_train_small, y_train_small)\n","\n","# Best parameters from Random Search\n","best_params = random_search.best_params_\n","print(\"Best parameters from Random Search:\", best_params)\n","\n","# Define the parameter grid based on RandomizedSearchCV results\n","param_grid = {\n","    'n_estimators': [best_params['n_estimators'] - 100, best_params['n_estimators'], best_params['n_estimators'] + 100],\n","    'max_depth': [best_params['max_depth'] - 3, best_params['max_depth'], best_params['max_depth'] + 3],\n","    'learning_rate': [best_params['learning_rate'] / 2, best_params['learning_rate'], best_params['learning_rate'] * 2],\n","    'subsample': [max(0.6, best_params['subsample'] - 0.1), best_params['subsample'], min(1.0, best_params['subsample'] + 0.1)],\n","    'colsample_bytree': [max(0.6, best_params['colsample_bytree'] - 0.1), best_params['colsample_bytree'], min(1.0, best_params['colsample_bytree'] + 0.1)]\n","}\n","\n","# GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters from Grid Search\n","best_params = grid_search.best_params_\n","print(\"Best parameters from Grid Search:\", best_params)\n","\n","# Train model with best parameters on full dataset\n","model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', tree_method='hist', device='cuda', use_label_encoder=False)\n","model.fit(X_train, y_train)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_val)\n","rmse = mean_squared_error(y_val, y_pred, squared=False)  # squared=False to get RMSE directly\n","print(f'Validation RMSE: {rmse}')\n","\n","# Cross-validation\n","cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n","cv_rmse = np.sqrt(-cv_scores)\n","print(f'Cross-validated RMSE: {cv_rmse.mean()}')\n","\n","# Make predictions on the test data\n","predictions = model.predict(test_df)  # Drop 'id' column for prediction\n","predictions = np.expm1(predictions)  # Inverse log transformation to get back to original scale\n","\n","submission_df = pd.DataFrame({\n","    'id': test_ids,  # Use the stored 'id' column\n","    'price': predictions\n","})\n","\n","# Create submission file\n","submission_df.to_csv('submission.csv', index=False)\n","\n","print('Submission file created: submission.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAJ0Brf4MotU","executionInfo":{"status":"ok","timestamp":1717243289356,"user_tz":-330,"elapsed":18771871,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"3344ff21-7583-4a37-a33d-5a6c90fb15ce"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 50 candidates, totalling 150 fits\n","Best parameters from Random Search: {'subsample': 0.6, 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n","Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best parameters from Grid Search: {'colsample_bytree': 0.6, 'learning_rate': 0.02, 'max_depth': 12, 'n_estimators': 600, 'subsample': 0.7}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:58:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","Potential solutions:\n","- Use a data structure that matches the device ordinal in the booster.\n","- Set the device for booster before call to inplace_predict.\n","\n","This warning will only be shown once.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Validation RMSE: 0.48472387085826185\n","Cross-validated RMSE: 0.4958397684834079\n","Submission file created: submission.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7IT7mAL2zcSE","executionInfo":{"status":"aborted","timestamp":1717223571682,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files.download('/content/submission.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"eBvGG9pl-S8Y","executionInfo":{"status":"ok","timestamp":1717245451080,"user_tz":-330,"elapsed":681,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"fad7ad22-4948-4524-fe75-d67c95ebae1f"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_c1311122-6022-4cd4-bcf3-c12bd1a528f1\", \"submission.csv\", 565339)"]},"metadata":{}}]},{"cell_type":"code","source":["\n","#This portion is to plot the dsitribution, check for skewness, outliers etc\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n"],"metadata":{"id":"QPTSnie637_C","executionInfo":{"status":"aborted","timestamp":1717223571683,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#This portion is to plot the dsitribution, check for skewness, outliers etc\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Measure skewness\n","skewness = prices.skew()\n","print(f'Skewness of the target variable (price): {skewness}')\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n","\n","# Display the statistics for the boxplot\n","Q1 = prices.quantile(0.25)\n","Q3 = prices.quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(f'Q1 (25th percentile): {Q1}')\n","print(f'Q3 (75th percentile): {Q3}')\n","print(f'IQR (Interquartile Range): {IQR}')\n","print(f'Lower bound for outliers: {lower_bound}')\n","print(f'Upper bound for outliers: {upper_bound}')\n","print(f'Number of outliers below lower bound: {(prices < lower_bound).sum()}')\n","print(f'Number of outliers above upper bound: {(prices > upper_bound).sum()}')\n"],"metadata":{"id":"5LFPqi545RVU","executionInfo":{"status":"aborted","timestamp":1717223571683,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This portion is to plot the dsitribution, calculate skewness, outliers etc\n","\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the dataset\n","train_df = pd.read_csv('/content/train.csv')\n","\n","# Extract the target variable\n","prices = train_df['price']\n","\n","# Measure skewness\n","skewness = prices.skew()\n","print(f'Skewness of the target variable (price): {skewness}')\n","\n","# Generate basic statistics\n","price_stats = prices.describe()\n","print(price_stats)\n","\n","# Plot the distribution of the target variable\n","plt.figure(figsize=(10, 6))\n","sns.histplot(prices, bins=50, kde=True)\n","plt.title('Distribution of Prices')\n","plt.xlabel('Price')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Plot a boxplot to detect outliers\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=prices)\n","plt.title('Boxplot of Prices')\n","plt.xlabel('Price')\n","plt.show()\n","\n","# Display the statistics for the boxplot\n","Q1 = prices.quantile(0.25)\n","Q3 = prices.quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","print(f'Q1 (25th percentile): {Q1}')\n","print(f'Q3 (75th percentile): {Q3}')\n","print(f'IQR (Interquartile Range): {IQR}')\n","print(f'Lower bound for outliers: {lower_bound}')\n","print(f'Upper bound for outliers: {upper_bound}')\n","print(f'Number of outliers below lower bound: {(prices < lower_bound).sum()}')\n","print(f'Number of outliers above upper bound: {(prices > upper_bound).sum()}')\n","\n","# Apply log transformation to reduce skewness\n","train_df['price'] = np.log1p(train_df['price'])\n","\n","# Ensure to also apply the transformation to the validation set if needed\n","# If 'price' column in validation set (if any)\n","# validation_df['price'] = np.log1p(validation_df['price'])\n","\n","# Check the new distribution after log transformation\n","plt.figure(figsize=(10, 6))\n","sns.histplot(train_df['price'], bins=50, kde=True)\n","plt.title('Distribution of Log-Transformed Prices')\n","plt.xlabel('Log-Transformed Price')\n","plt.ylabel('Frequency')\n","plt.show()\n"],"metadata":{"id":"03TIza_Z6CwG","executionInfo":{"status":"aborted","timestamp":1717223571683,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R5vzDzrBRv2E","executionInfo":{"status":"aborted","timestamp":1717223571683,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace with your GitHub email and username\n","!git config --global user.email \"deepanshanmugam13@gmail.com\"\n","!git config --global user.name \"Dheep13\"\n"],"metadata":{"id":"JuS-V518SYnx","executionInfo":{"status":"ok","timestamp":1717245918731,"user_tz":-330,"elapsed":760,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/Dheep13/KaggleContest.git"],"metadata":{"id":"WWJuAEQXS199","executionInfo":{"status":"aborted","timestamp":1717223571683,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","dataset_dir = '/content'\n","for dirname, _, filenames in os.walk(dataset_dir):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B9CUs5TTGXH","executionInfo":{"status":"ok","timestamp":1717245924765,"user_tz":-330,"elapsed":669,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"a6773aa5-b867-475a-b85f-b0e0c970a65f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/train.csv\n","/content/kagglex-cohort4.zip\n","/content/submission.csv\n","/content/test.csv\n","/content/kaggle.json\n","/content/sample_submission.csv\n","/content/.config/gce\n","/content/.config/config_sentinel\n","/content/.config/default_configs.db\n","/content/.config/.last_update_check.json\n","/content/.config/.last_opt_in_prompt.yaml\n","/content/.config/active_config\n","/content/.config/.last_survey_prompt.yaml\n","/content/.config/logs/2024.05.30/13.23.26.998698.log\n","/content/.config/logs/2024.05.30/13.36.15.385607.log\n","/content/.config/logs/2024.05.30/13.30.33.059431.log\n","/content/.config/logs/2024.05.30/13.30.21.274210.log\n","/content/.config/logs/2024.05.30/13.36.16.059291.log\n","/content/.config/logs/2024.05.30/13.36.03.155708.log\n","/content/.config/configurations/config_default\n","/content/sample_data/anscombe.json\n","/content/sample_data/README.md\n","/content/sample_data/california_housing_test.csv\n","/content/sample_data/california_housing_train.csv\n","/content/sample_data/mnist_test.csv\n","/content/sample_data/mnist_train_small.csv\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Configure Git\n","!git config --global user.email \"deepanshanmugam13@gmail.com\"\n","!git config --global user.name \"Dheep13\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ew95ltc_Tsna","executionInfo":{"status":"ok","timestamp":1717245965649,"user_tz":-330,"elapsed":30892,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"12adfff1-c287-4712-8a20-e09db15f5880"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Clone your repository\n","# !git clone https://github.com/Dheep13/KaggleContest.git\n","import os\n","\n","# Check if the repository directory exists\n","if os.path.isdir('/KaggleContest/assignments'):\n","    print(\"Repository successfully cloned.\")\n","else:\n","    print(\"Failed to clone repository.\")\n","\n","# !mkdir -p /content/KaggleContest/assignments\n","\n","\n","# Path to the notebook in Google Drive\n","notebook_path = '/content/drive/My Drive/Colab Notebooks/KaggleX_RandomSCV&GridSCV_Ver3.ipynb'\n","\n","# # Copy the notebook file to the repository directory\n","!cp \"{notebook_path}\" \"./KaggleContest/assignments/KaggleX_RandomSCV&GridSCV_Ver3.ipynb\"\n","\n"],"metadata":{"id":"InK2MZs_QbCA","executionInfo":{"status":"ok","timestamp":1717247233820,"user_tz":-330,"elapsed":646,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d70a814-4bca-4c62-857b-63733184891f"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to clone repository.\n"]}]},{"cell_type":"code","source":["# Change directory to your repository\n","%cd /KaggleContest/assignments\n","!ls -R\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7CRyAJTQfe7","executionInfo":{"status":"ok","timestamp":1717247112170,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"outputId":"e68e9ed6-78c1-4135-d7c5-b3f0f7f71c56"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/KaggleContest\n",".:\n","assignments  KaggleContest\n","\n","./assignments:\n","'KaggleX_RandomSCV&GridSCV_Ver3.ipynb'\n","\n","./KaggleContest:\n","assignments  KaggleX_Ver2.ipynb  README.md\n","\n","./KaggleContest/assignments:\n","'Artificial Neural Networks Competition-deepan.ipynb'\t     Linear_Regression-deepan.ipynb\n"," binary-classification-competition-deepan.ipynb\t\t     Multiclass_Classification-deepan.ipynb\n","'BirdClassification using Transformers-final-deepan.ipynb'   RAG_Test_Gamma.ipynb\n","'Copy of Regression with ANNs-deepan.ipynb'\n"]}]},{"cell_type":"code","source":["# Add the notebook file to the staging area\n","!git add assignments/'KaggleX_RandomSCV&GridSCV_Ver3.ipynb'\n","\n"],"metadata":{"id":"_U-dOD56QhbR","executionInfo":{"status":"ok","timestamp":1717247104081,"user_tz":-330,"elapsed":642,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"46787d1d-a294-47d9-ea4d-99a807a5f148"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n"]}]},{"cell_type":"code","source":["# Commit the changes\n","!git commit -m \"KaggleX_Ver2.ipynb 27th place code\"\n","\n"],"metadata":{"id":"VvaGMw7PVnU-","executionInfo":{"status":"aborted","timestamp":1717223571684,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Push the changes to the remote repository\n","# Replace 'https://yourusername:yourtoken@github.com/Dheep13/KaggleContest.git' with your actual repository URL\n","!git push https://Dheep13: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@github.com/Dheep13/KaggleContest.git\n","# !git push -u origin main"],"metadata":{"id":"1EAddLdwVquP","executionInfo":{"status":"aborted","timestamp":1717223571684,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deepan Shanmugam","userId":"10513139089640654961"}}},"execution_count":null,"outputs":[]}]}